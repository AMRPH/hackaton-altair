{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 20:45:52 WARN Utils: Your hostname, MacBook-Pro-Danil.local resolves to a loopback address: 127.0.0.1; using 192.168.0.131 instead (on interface en0)\n",
      "24/08/27 20:45:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/27 20:45:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/08/27 20:45:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m\n\u001b[1;32m     81\u001b[0m filter_ids \u001b[38;5;241m=\u001b[39m filter_ids\u001b[38;5;241m.\u001b[39mfilter(filter_ids\u001b[38;5;241m.\u001b[39mlag \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lag\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnpo_account_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m ids \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(filter_ids\u001b[38;5;241m.\u001b[39mcollect())\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 84\u001b[0m dataset_pl \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m# 80% времени выполняется эта строчка\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[0;32m~/Library/Python/3.12/lib/python/site-packages/pyspark/sql/pandas/utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/27 20:46:04 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "pl.Config(tbl_rows=100)\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "        .master(\"local[6]\")\n",
    "        .appName('PySpark_Prepare')\n",
    "        .config(\"spark.driver.memory\", \"15g\")\n",
    "        .getOrCreate())\n",
    "\n",
    "dataset_schema = [\n",
    "    T.StructField('slctn_nmbr', T.IntegerType(), True),\n",
    "    T.StructField('client_id', T.StringType(), True),\n",
    "    T.StructField('npo_account_id', T.StringType(), True),\n",
    "    T.StructField('npo_accnts_nmbr', T.IntegerType(), True),\n",
    "    T.StructField('pmnts_type', T.IntegerType(), True),\n",
    "    T.StructField('year', T.IntegerType(), True),\n",
    "    T.StructField('quarter', T.StringType(), True),\n",
    "    T.StructField('gender', T.IntegerType(), True),\n",
    "    T.StructField('age', T.IntegerType(), True),\n",
    "    T.StructField('clnt_cprtn_time_d', T.IntegerType(), True),\n",
    "    T.StructField('actv_prd_d', T.IntegerType(), True),\n",
    "    T.StructField('lst_pmnt_rcnc_d', T.IntegerType(), True),\n",
    "    T.StructField('balance', T.FloatType(), True),\n",
    "    T.StructField('oprtn_sum_per_qrtr', T.FloatType(), True),\n",
    "    T.StructField('oprtn_sum_per_year', T.FloatType(), True),\n",
    "    T.StructField('frst_pmnt_date', T.StringType(), True),\n",
    "    T.StructField('lst_pmnt_date_per_qrtr', T.IntegerType(), True),\n",
    "    T.StructField('frst_pmnt', T.FloatType(), True),\n",
    "    T.StructField('lst_pmnt', T.FloatType(), True),\n",
    "    T.StructField('pmnts_sum', T.FloatType(), True),\n",
    "    T.StructField('pmnts_nmbr', T.IntegerType(), True),\n",
    "    T.StructField('pmnts_sum_per_qrtr', T.FloatType(), True),\n",
    "    T.StructField('pmnts_sum_per_year', T.FloatType(), True),\n",
    "    T.StructField('pmnts_nmbr_per_qrtr', T.IntegerType(), True),\n",
    "    T.StructField('pmnts_nmbr_per_year', T.IntegerType(), True),\n",
    "    T.StructField('incm_sum', T.FloatType(), True),\n",
    "    T.StructField('incm_per_qrtr', T.FloatType(), True),\n",
    "    T.StructField('incm_per_year', T.FloatType(), True),\n",
    "    T.StructField('mgd_accum_period', T.FloatType(), True),\n",
    "    T.StructField('mgd_payment_period', T.FloatType(), True),\n",
    "    T.StructField('phone_number', T.IntegerType(), True),\n",
    "    T.StructField('email', T.IntegerType(), True),\n",
    "    T.StructField('lk', T.IntegerType(), True),\n",
    "    T.StructField('assignee_npo', T.IntegerType(), True),\n",
    "    T.StructField('assignee_ops', T.IntegerType(), True),\n",
    "    T.StructField('postal_code', T.StringType(), True),\n",
    "    T.StructField('region', T.StringType(), True),\n",
    "    T.StructField('citizen', T.IntegerType(), True),\n",
    "    T.StructField('fact_addrss', T.IntegerType(), True),\n",
    "    T.StructField('appl_mrkr', T.IntegerType(), True),\n",
    "    T.StructField('evry_qrtr_pmnt', T.IntegerType(), True),\n",
    "    T.StructField('churn', T.IntegerType(), True)\n",
    "]\n",
    "\n",
    "dataset_struct = T.StructType(fields=dataset_schema)\n",
    "\n",
    "dataset = spark.read.csv('dataset/train.csv', sep=',', header=True, schema=dataset_struct)\n",
    "# удаляем ненужные параметры\n",
    "dataset = dataset.drop(*['oprtn_sum_per_year', 'frst_pmnt_date', 'lst_pmnt_date_per_qrtr', 'pmnts_sum_per_year',\n",
    "                        'pmnts_nmbr_per_year', 'incm_per_year', 'postal_code', 'npo_accnts_nmbr',\n",
    "                        'slctn_nmbr', 'client_id'])\n",
    "dataset = dataset.dropDuplicates()\n",
    "dataset = dataset.dropna()\n",
    "# сортируем\n",
    "dataset = dataset.sort(['npo_account_id', 'quarter'])\n",
    "\n",
    "lag = 8\n",
    "# данные для обогощения датасета\n",
    "data_cntrbtrs = pl.read_csv('dataset/cntrbtrs.csv', separator=';')\n",
    "region_encoder = pl.read_csv('dataset/region_encoder.csv')\n",
    "gdp = pl.read_csv('dataset/gdp.csv')\n",
    "mrot = pl.read_csv('dataset/mrot.csv')\n",
    "usd = pl.read_csv('dataset/usd.csv')\n",
    "\n",
    "# оставляем только счета с >= 8+1 записями\n",
    "filter_ids = dataset.groupBy('npo_account_id').count().withColumnRenamed('count', 'lag')\n",
    "filter_ids = filter_ids.filter(filter_ids.lag >= lag+1).select('npo_account_id')\n",
    "ids = np.array(filter_ids.collect()).reshape(-1)\n",
    "\n",
    "dataset_pl = pl.from_pandas(dataset.toPandas())# 80% времени выполняется эта строчка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config(tbl_rows=1000)\n",
    "\n",
    "column_names = list()\n",
    "# кодирование региона\n",
    "def get_region(region):\n",
    "    region = region.split(' ')[0]\n",
    "    new_value = region_encoder.filter(pl.col('region') == region)['value'][0]\n",
    "    return new_value\n",
    "\n",
    "# значение мрот rub\n",
    "def get_mrot(year):\n",
    "    new_value = mrot.filter(pl.col('year') == year)['rubles'][0]\n",
    "    return int(new_value)\n",
    "\n",
    "# значение ввп на душу населения usd\n",
    "def get_gdp(year):\n",
    "    new_value = gdp.filter(pl.col('year') == year)['usd'][0]\n",
    "    return int(new_value)\n",
    "\n",
    "# значение курса доллара\n",
    "def get_usd(quarter):\n",
    "    new_value = usd.filter(pl.col('quarter') == quarter)['rubles'][0]\n",
    "    return new_value\n",
    "\n",
    "# тип пенсионного вклада\n",
    "def get_pens_type(id):\n",
    "    value = data_cntrbtrs.filter(pl.col('npo_accnt_id') == id)['accnt_pnsn_schm'][0]\n",
    "    return value\n",
    "\n",
    "\n",
    "new_dataset = []\n",
    "for i in range(len(ids)):\n",
    "    id = ids[i]\n",
    "    df_id = dataset_pl.filter(pl.col('npo_account_id') == id) # берем записи конкретного счета\n",
    "    df_id = df_id.drop('npo_account_id')\n",
    "\n",
    "    target = df_id.tail(1)['churn'][0]# сохраняем таргет и тип вклада\n",
    "    pens = get_pens_type(id)\n",
    "\n",
    "    df_id = df_id.drop('churn')\n",
    "    df_id = df_id.slice(len(df_id)-lag-1, lag)# оставляем последние 8 отчетов\n",
    "\n",
    "    df_id = df_id.with_columns(pl.col('region')\n",
    "                                        .map_elements(lambda x: get_region(x), return_dtype=pl.Float64)\n",
    "                                        .alias('region'))# кодируем регион\n",
    "    \n",
    "    df_id = df_id.with_columns(pl.col('quarter')\n",
    "                                        .map_elements(lambda x: get_usd(x), return_dtype=pl.Float64)\n",
    "                                        .alias('usd'))# добавляем курс доллара rub\n",
    "    \n",
    "\n",
    "    df_id = df_id.with_columns(pl.col('year')\n",
    "                                        .map_elements(lambda x: get_mrot(x), return_dtype=pl.Int64)\n",
    "                                        .alias('mrot_rub'))# добавляем мрот rub\n",
    "    \n",
    "    df_id = df_id.with_columns((pl.col('mrot_rub') / pl.col('usd'))\n",
    "                                         .alias('mrot_usd'))# добавляем мрот usd\n",
    "    \n",
    "\n",
    "    df_id = df_id.with_columns(pl.col('year')\n",
    "                                        .map_elements(lambda x: get_gdp(x), return_dtype=pl.Int64)\n",
    "                                        .alias('gdp_usd'))# добавляем ввп usd\n",
    "\n",
    "    \n",
    "    df_id = df_id.with_columns((pl.col('gdp_usd') * pl.col('usd'))\n",
    "                                         .alias('gdp_rub'))# добавляем ввп rub\n",
    "    \n",
    "    \n",
    "    df_id = df_id.with_columns(pl.col('quarter')\n",
    "                                        .map_elements(lambda x: int(x[5:6]), return_dtype=pl.Int64)\n",
    "                                        .alias('quarter'))\n",
    "    df_id = df_id.drop('year')\n",
    "\n",
    "    if (len(column_names) == 0):\n",
    "        df_names = df_id.columns\n",
    "        for i in range(lag):\n",
    "            for j in range(len(df_names)):\n",
    "                column_names.append(df_names[j] + '_' + str(i+1))\n",
    "        column_names.append('pens_type')\n",
    "        column_names.append('target')\n",
    "\n",
    "    data_id = np.append(df_id.to_numpy().reshape(-1), (pens, target))\n",
    "    new_dataset.append(data_id)\n",
    "    print(len(new_dataset))\n",
    "np.save('dataset_train_prepared.npy', np.array(new_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
